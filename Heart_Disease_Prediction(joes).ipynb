{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenBuchanan2001/BenBuchanan2001/blob/main/Heart_Disease_Prediction(joes).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data and preproccessing data"
      ],
      "metadata": {
        "id": "5t89dQDSY2Ln"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RIPnsSjYE05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "12d826bc-2208-49e8-b5b5-3f3540484f25"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Heart_Disease_Prediction.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c6c832794c8c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Heart_Disease_Prediction.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_analyse_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;31m# all datatypes are either int64 or float64 apart from the the target attribute \"Heart Disease\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# The only variable that needs onehotencoding is the target variable and because the only two values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-c6c832794c8c>\u001b[0m in \u001b[0;36mload_and_analyse_data\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m \"\"\"\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_analyse_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Data head: \\n {raw_data.head()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Heart_Disease_Prediction.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib  # For saving the scaler\n",
        "\n",
        "\"\"\"\n",
        "Team Member: Ben Buchanan\n",
        "What the code does:\n",
        "- Loads data from a CSV file (using the given file path) into a pandas DataFrame.\n",
        "- Prints the first few rows to give a preview of the data.\n",
        "- Iterates over each column to print its name and datatype.\n",
        "- Displays descriptive statistics for the \"Heart Disease\" column.\n",
        "- Returns the loaded DataFrame for further analysis.\n",
        "\"\"\"\n",
        "def load_and_analyse_data(data_path):\n",
        "    raw_data = pd.read_csv(data_path)\n",
        "\n",
        "    print(f\"Data head: \\n {raw_data.head()}\")\n",
        "\n",
        "    for column, dtype in raw_data.dtypes.items():\n",
        "        print(f\"Column name: {column}, Datatype: {dtype}\")\n",
        "\n",
        "    print(raw_data[\"Heart Disease\"].describe())\n",
        "\n",
        "    return raw_data\n",
        "\n",
        "\n",
        "def categorise(row):\n",
        "    \"\"\"\n",
        "    My simple categorisation function:\n",
        "    \"\"\"\n",
        "    if row[\"Heart Disease\"] == \"Presence\":\n",
        "        return 1\n",
        "\n",
        "    elif row[\"Heart Disease\"] == \"Absence\":\n",
        "        return 0\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unexpected value: {row['Heart Disease']}\")\n",
        "\n",
        "\n",
        "def preprocess_data(raw_data, save_scalar=True, scalar_path=\"scalars/scaler.pkl\"):\n",
        "\n",
        "    X = raw_data.drop([\"Heart Disease\", \"Target\"], axis=1)\n",
        "    y = raw_data[\"Target\"]\n",
        "\n",
        "    scalar = StandardScaler()\n",
        "\n",
        "    X_scaled = pd.DataFrame(scalar.fit_transform(X), columns=X.columns, index=X.index)\n",
        "\n",
        "    if save_scalar:\n",
        "        os.makedirs(os.path.dirname(scalar_path), exist_ok=True)\n",
        "        joblib.dump(scalar, scalar_path)\n",
        "\n",
        "    return X_scaled, y\n",
        "\n",
        "\n",
        "data_path = \"Heart_Disease_Prediction.csv\"\n",
        "\n",
        "raw_data = load_and_analyse_data(data_path)\n",
        "# all datatypes are either int64 or float64 apart from the the target attribute \"Heart Disease\"\n",
        "# The only variable that needs onehotencoding is the target variable and because the only two values\n",
        "# are either absense or presense I think it will be quicker to just write a onehotencoder myself:\n",
        "\n",
        "raw_data[\"Target\"] = raw_data.apply(lambda row: categorise(row), axis=1)\n",
        "print(raw_data)\n",
        "\n",
        "X, y = preprocess_data(raw_data)\n",
        "\n",
        "## inspect the data:\n",
        "print(\"Processed X:\")\n",
        "print(X)\n",
        "print(\"Processed y:\")\n",
        "print(y)\n",
        "\n",
        "output_dir = \"preprocessed_data\"\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "X.to_csv(output_dir + \"/X.csv\")\n",
        "y.to_csv(output_dir + \"/y.csv\")\n",
        "\n",
        "print(f\"Scaled data saved to {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split data into train and test"
      ],
      "metadata": {
        "id": "fV1Y54IXPXyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=22)"
      ],
      "metadata": {
        "id": "4C83iiClKCTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition"
      ],
      "metadata": {
        "id": "ZPLkQP9ZPyzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class HeartDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "            if isinstance(features, pd.DataFrame):\n",
        "                self.features = torch.tensor(features.values, dtype=torch.float32)\n",
        "            else:\n",
        "                self.features = torch.tensor(features, dtype=torch.float32)\n",
        "\n",
        "            if isinstance(labels, pd.Series):\n",
        "                self.labels = torch.tensor(labels.values, dtype=torch.float32).reshape(-1, 1)\n",
        "            else:\n",
        "                self.labels = torch.tensor(labels, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "class HeartDiseasePredictionModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(HeartDiseasePredictionModel, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, patience=10):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    train_loss_list = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_loss_list.append(train_loss)\n",
        "\n",
        "\n",
        "        # val phase\n",
        "\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_val_loss += loss.item()\n",
        "\n",
        "        val_loss = running_val_loss / len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    return model, train_loss_list, val_losses\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "muRP6dKPPwcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "og-yNcx-UaUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wjopHrMHUaoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rsJcIORMUa3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7LXtctwrUQV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = len(X_train.columns) if isinstance(X_train, pd.DataFrame) else X_train.shape[1]\n",
        "\n",
        "train_dataset = HeartDataset(X_train, y_train)\n",
        "test_dataset = HeartDataset(X_test, y_test)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "model = HeartDiseasePredictionModel(input_dim)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "trained_model, train_losses, val_losses = train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    test_loader,\n",
        "    criterion,\n",
        "    optimizer\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NT41-Sc_QWVq",
        "outputId": "0fbe1ac6-38fa-4fa8-bc37-46a94de49316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6929, Val Loss: 0.6788\n",
            "Epoch 2/100, Train Loss: 0.6874, Val Loss: 0.6737\n",
            "Epoch 3/100, Train Loss: 0.6809, Val Loss: 0.6688\n",
            "Epoch 4/100, Train Loss: 0.6767, Val Loss: 0.6639\n",
            "Epoch 5/100, Train Loss: 0.6681, Val Loss: 0.6588\n",
            "Epoch 6/100, Train Loss: 0.6633, Val Loss: 0.6531\n",
            "Epoch 7/100, Train Loss: 0.6566, Val Loss: 0.6465\n",
            "Epoch 8/100, Train Loss: 0.6472, Val Loss: 0.6387\n",
            "Epoch 9/100, Train Loss: 0.6382, Val Loss: 0.6299\n",
            "Epoch 10/100, Train Loss: 0.6300, Val Loss: 0.6198\n",
            "Epoch 11/100, Train Loss: 0.6188, Val Loss: 0.6086\n",
            "Epoch 12/100, Train Loss: 0.6082, Val Loss: 0.5965\n",
            "Epoch 13/100, Train Loss: 0.5923, Val Loss: 0.5833\n",
            "Epoch 14/100, Train Loss: 0.5746, Val Loss: 0.5681\n",
            "Epoch 15/100, Train Loss: 0.5543, Val Loss: 0.5503\n",
            "Epoch 16/100, Train Loss: 0.5385, Val Loss: 0.5292\n",
            "Epoch 17/100, Train Loss: 0.5197, Val Loss: 0.5064\n",
            "Epoch 18/100, Train Loss: 0.4925, Val Loss: 0.4821\n",
            "Epoch 19/100, Train Loss: 0.4681, Val Loss: 0.4590\n",
            "Epoch 20/100, Train Loss: 0.4501, Val Loss: 0.4367\n",
            "Epoch 21/100, Train Loss: 0.4302, Val Loss: 0.4171\n",
            "Epoch 22/100, Train Loss: 0.4129, Val Loss: 0.4005\n",
            "Epoch 23/100, Train Loss: 0.3978, Val Loss: 0.3858\n",
            "Epoch 24/100, Train Loss: 0.3861, Val Loss: 0.3733\n",
            "Epoch 25/100, Train Loss: 0.3814, Val Loss: 0.3631\n",
            "Epoch 26/100, Train Loss: 0.3670, Val Loss: 0.3552\n",
            "Epoch 27/100, Train Loss: 0.3596, Val Loss: 0.3481\n",
            "Epoch 28/100, Train Loss: 0.3524, Val Loss: 0.3418\n",
            "Epoch 29/100, Train Loss: 0.3488, Val Loss: 0.3369\n",
            "Epoch 30/100, Train Loss: 0.3449, Val Loss: 0.3327\n",
            "Epoch 31/100, Train Loss: 0.3494, Val Loss: 0.3297\n",
            "Epoch 32/100, Train Loss: 0.3334, Val Loss: 0.3266\n",
            "Epoch 33/100, Train Loss: 0.3385, Val Loss: 0.3239\n",
            "Epoch 34/100, Train Loss: 0.3342, Val Loss: 0.3223\n",
            "Epoch 35/100, Train Loss: 0.3275, Val Loss: 0.3212\n",
            "Epoch 36/100, Train Loss: 0.3273, Val Loss: 0.3196\n",
            "Epoch 37/100, Train Loss: 0.3285, Val Loss: 0.3186\n",
            "Epoch 38/100, Train Loss: 0.3321, Val Loss: 0.3164\n",
            "Epoch 39/100, Train Loss: 0.3209, Val Loss: 0.3167\n",
            "Epoch 40/100, Train Loss: 0.3233, Val Loss: 0.3159\n",
            "Epoch 41/100, Train Loss: 0.3175, Val Loss: 0.3155\n",
            "Epoch 42/100, Train Loss: 0.3171, Val Loss: 0.3150\n",
            "Epoch 43/100, Train Loss: 0.3151, Val Loss: 0.3148\n",
            "Epoch 44/100, Train Loss: 0.3139, Val Loss: 0.3137\n",
            "Epoch 45/100, Train Loss: 0.3194, Val Loss: 0.3130\n",
            "Epoch 46/100, Train Loss: 0.3128, Val Loss: 0.3118\n",
            "Epoch 47/100, Train Loss: 0.3119, Val Loss: 0.3119\n",
            "Epoch 48/100, Train Loss: 0.3130, Val Loss: 0.3116\n",
            "Epoch 49/100, Train Loss: 0.3053, Val Loss: 0.3113\n",
            "Epoch 50/100, Train Loss: 0.3059, Val Loss: 0.3111\n",
            "Epoch 51/100, Train Loss: 0.3079, Val Loss: 0.3114\n",
            "Epoch 52/100, Train Loss: 0.3043, Val Loss: 0.3108\n",
            "Epoch 53/100, Train Loss: 0.3028, Val Loss: 0.3105\n",
            "Epoch 54/100, Train Loss: 0.2980, Val Loss: 0.3107\n",
            "Epoch 55/100, Train Loss: 0.3033, Val Loss: 0.3113\n",
            "Epoch 56/100, Train Loss: 0.3023, Val Loss: 0.3114\n",
            "Epoch 57/100, Train Loss: 0.2954, Val Loss: 0.3127\n",
            "Epoch 58/100, Train Loss: 0.3003, Val Loss: 0.3125\n",
            "Epoch 59/100, Train Loss: 0.2955, Val Loss: 0.3120\n",
            "Epoch 60/100, Train Loss: 0.2896, Val Loss: 0.3125\n",
            "Epoch 61/100, Train Loss: 0.2959, Val Loss: 0.3133\n",
            "Epoch 62/100, Train Loss: 0.2916, Val Loss: 0.3128\n",
            "Epoch 63/100, Train Loss: 0.2914, Val Loss: 0.3124\n",
            "Early stopping at epoch 63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model evaluation"
      ],
      "metadata": {
        "id": "KiOAzgF9Uj36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "TDC2wOKxUllD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}